---
title: "Weight Lifting Execise Detection"
author: "Marcelo Guimaraes"
date: "3 de agosto de 2016"
output: html_document
---
```{r,echo=FALSE}
library(RefManageR)
bib <- ReadBib("document.bib", check = FALSE)
BibOptions(check.entries = FALSE, style = "markdown", bib.style = "numeric", cite.style = 'numeric')
```
##Introduction:

This document is part of a final project of the machine learning class provided by the John Hopkins university through the Coursera plataform. 
The maching learning in this document is applied in the context of Human Activity Recognition (HAR). More specifically, it is applied in the recognition of the performance of 5 classes of weight lifting exercises from 6 healthy subjects. The goal of this assignment is to create an algorithm that is able to identify the exercise class performed, and to estimate the expected out of sample error. The training dataset used in this document was obtained from this [link](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). 
The resulting algorithm will be applied to 20 different [test cases ](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).
These results were obtained from  `r Citet(bib, 1)`.

### Review
In this study[1] the subjects were asked to perform barbell lifts correctly and incorrectly in 5 different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
The execises were captured by 4 sensors (arm,belt,forearm and dumbbell). Each sensor provided data of 13 features [three-axis (x,y,z) for acceleration , gyroscope, and magnetometer + three Euler angles (roll, pitch and yaw) + acceleration module], therefore 52 features were collected. One adicional collumn of the dataset is the way (class) the exercise was performed. For the Euler angles, additional features were then calculated:  mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness, generating in total 96 angle derived feature sets plus 4 features for the variance of the total acceleration.
They used Mark Hall algorithm with "Best First" strategy for feature selection. 17 features were selected:

>Selected Features

* belt
    + roll
        * mean and variance
    + acelerometer module
        * maximum, amplitude and variance
    + gyro
        * variance
    + magnetometer
        * variance 
* arm
    + accelerometer
        * variance
    + magnetometer            
        * maximum and minimum
* dumbbell
    + acceleration
        * maximum
    + gyro
        * variance
    + magnetometer
        * maximum and minimum
* forearm
    + pitch
        * sum
    + gyro
        * maximum and minimum

They combined the result ("Bagging") of 10 random forest used for classification with each forest implemented with 10 trees. The classifier was tested with a 10-fold cross-validation. The overall recognition performance was of 98.2% (Weighted average). With leave-one-subject-out validation test the accuraccy is much lower (78.2%) due to the small size of the database. 
# Processing Data

## Downloading Data
```{r}
#training=download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","trainingraw.csv")
#testing=download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","testing.csv")
```

## Cleaning Data
```{r}
#command="sed 's:\"\"::g' trainingraw.csv > training.csv"
#system(command)
```

## Loading Data

```{r}
library(data.table)
vec=rep("numeric",160)
whichar=1:7
vec[whichar]=rep("character",length(whichar))
vec[160]="factor"
library(data.table)
trainingraw=data.table(read.csv("training.csv",na.strings=c("N",NA,"", "#DIV/0!"),stringsAsFactors = F))
testingraw=data.table(read.csv("testing.csv",na.strings=c("N",NA,"", "#DIV/0!"),stringsAsFactors = F))
#replacing incorrectly named column 16: skewness_roll_belt.1
colnames(trainingraw)[16] = "skewness_pitch_belt"
```


## Imputing Data

The training data set has the following dimensions: `r dim(trainingraw)`,
we can see that this set is a subsample from the original set with 39243 rows.
Most of the columns of this dataset are incomplete,  special attention must be given for imputing those columns since only those columns are selected for classification (see Selected Features at Introduction):

```{r}
 incompleteness=unlist(lapply(1:160,function(col){sum(is.na(trainingraw[[col]]))}))/dim(trainingraw)[1]*100
 incomplete.dt=data.table(percentage=incompleteness,description=colnames(trainingraw))
 highly.incomplete.columns=sum(incomplete.dt$percentage >97)
 complete.columns=sum(incomplete.dt$percentage == 0)
```

Only `r complete.columns` columns are completely filled with data. An explanation for this incompleteness could not be found, however, the `r highly.incomplete.columns` incomplete columns are derived from the 52 measured features.  
Each calculation were performed within a time-window in which the exercise was performed with a 0.5 seconds overlap between windows. It is not clear whether the calculated values were obtained from unavailble data or if it were obtained from the data within the time-window defined in the dataset by the num_window column. In the former case the values can be recomputed, but in that case an exercise can only be identified if a whole time-window is provided (context). An isolated data point (test case) usually can not give enough information to infer if the exercise is being properly executed (similar to evaluating the performance of an exercise through a snapshot instead of a movie).
We can see that the necessary context is provided withing the testing dataset:
```{r}
setdiff(testingraw$num_window,trainingraw$num_window)
```
All the window numbers from the test set is contained within the training set, which is weird since 
the class can be completely determined by the window_num. The proper exercise would be to give a whole new window to calculate the derived features. Let's assume that we don't know the class of the test dataset, then we will imput the derived feature from the window obtained in the training set into the test set.
From the data we can see that 90% of the window size is about 0.9 seconds:
```{r}
explore=trainingraw
explore$pt=as.double(explore$raw_timestamp_part_1)+explore$raw_timestamp_part_2/1000000
h=hist(unique(explore[,.(pt,window_size=max(pt)-min(pt)),by=num_window])$window_size,breaks = 4)
data.table(percentage=h$counts/sum(h$counts)*100, window_size=h$mids)
```
From the literature the optimal window size is about 2.5 seconds. For optimal performance the window should be recalculated.
Let filter only the raw features from the dataset in order to calculate the derived columns:
```{r}
notderived=setdiff(1:160,grep("avg|std|var|kurt|skew|min|max|ampl",incomplete.dt$desc))
cnames=colnames(training)
belt=cnames|"belt"
beltroll=belt|"roll"|"(mean|var)"
beltaccer=belt|"acc"|"(max|amp|var)"
```
#Bibliography

```{r, results='asis',echo=FALSE}
PrintBibliography(bib)
```
