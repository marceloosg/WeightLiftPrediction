---
title: "Weight Lifting Execise Detection"
author: "Marcelo Guimaraes"
date: "3 de agosto de 2016"
output: html_document
---
```{r,echo=FALSE}
suppressPackageStartupMessages(library(RefManageR))
bib <- ReadBib("document.bib", check = FALSE)
BibOptions(check.entries = FALSE, style = "markdown", bib.style = "numeric", cite.style = 'numeric')
```
##Introduction:

This document is part of a final project of the machine learning class provided by the John Hopkins university through the Coursera plataform. 
The maching learning in this document is applied in the context of Human Activity Recognition (HAR). More specifically, it is applied in the recognition of the performance of 5 classes of weight lifting exercises from 6 healthy subjects. The goal of this assignment is to create an algorithm that is able to identify the exercise class performed, and to estimate the expected out of sample error. The training dataset used in this document was obtained from this [link](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv). 
The resulting algorithm will be applied to 20 different [test cases ](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).
These results were obtained from  `r Citet(bib, 1)`.

### Review
In this study[1] the subjects were asked to perform barbell lifts correctly and incorrectly in 5 different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
The execises were captured by 4 sensors (arm,belt,forearm and dumbbell). Each sensor provided data of 13 features [three-axis (x,y,z) for acceleration , gyroscope, and magnetometer + three Euler angles (roll, pitch and yaw) + acceleration module], therefore 52 features were collected. One adicional collumn of the dataset is the way (class) the exercise was performed. For the Euler angles, additional features were then calculated:  mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness, generating in total 96 angle derived feature sets plus 4 features for the variance of the total acceleration.
They used Mark Hall algorithm with "Best First" strategy for feature selection. 17 features were selected:

>Selected Features

* belt
    + roll
        * mean and variance
    + acelerometer module
        * maximum, amplitude and variance
    + gyro
        * variance
    + magnetometer
        * variance 
* arm
    + accelerometer
        * variance
    + magnetometer            
        * maximum and minimum
* dumbbell
    + acceleration
        * maximum
    + gyro
        * variance
    + magnetometer
        * maximum and minimum
* forearm
    + pitch
        * sum
    + gyro
        * maximum and minimum

They combined the result ("Bagging") of 10 random forest used for classification with each forest implemented with 10 trees. The classifier was tested with a 10-fold cross-validation. The overall recognition performance was of 98.2% (Weighted average). With leave-one-subject-out validation test the accuracy is much lower (78.2%) due to the small size of the database. 

# Processing Data

## Downloading Data
```{r}
if(!file.exists("./csv/trainingraw.csv")) {
training.status=download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","csv/trainingraw.csv")
}
if(!file.exists("./csv/testing.csv")) {
testing.status=download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","csv/testing.csv")
}
```

## Cleaning Data
```{r}
if(!file.exists("./csv/training.csv")) {
command="sed 's:\"\"::g' csv/trainingraw.csv > csv/training.csv"
system(command)
}
```

## Loading Data

```{r}
suppressPackageStartupMessages(library(data.table))
vec=rep("numeric",160)
whichar=1:7
vec[whichar]=rep("character",length(whichar))
vec[160]="factor"
trainingraw=data.table(read.csv("csv/training.csv",na.strings=c("N",NA,"", "#DIV/0!"),stringsAsFactors = F))
testingraw=data.table(read.csv("csv/testing.csv",na.strings=c("N",NA,"", "#DIV/0!"),stringsAsFactors = F))
#renaming incorrectly named column 16: skewness_roll_belt.1
colnames(trainingraw)[16] = "skewness_pitch_belt"
```


## Imputing Data

The training data set has the following dimensions: `r dim(trainingraw)`,
we can see that this set is a subsample from the original set with 39243 rows.
Most of the columns of this dataset are incomplete,  special attention must be given for imputing those columns since only those columns are selected for classification (see Selected Features at Introduction):

```{r}
 incompleteness=unlist(lapply(1:160,function(col){sum(is.na(trainingraw[[col]]))}))/dim(trainingraw)[1]*100
 incomplete.dt=data.table(percentage=incompleteness,description=colnames(trainingraw))
 highly.incomplete.columns=sum(incomplete.dt$percentage >97)
 complete.columns=sum(incomplete.dt$percentage == 0)
 h2=hist(incomplete.dt$percentage,  xlab="% of NA in a Column",
         ylab="# of columns",main="Histogram (sparse columns)")
```

Only `r complete.columns` columns are completely filled with data. An explanation for this incompleteness could not be found, however, the `r highly.incomplete.columns` highly incomplete columns (97% empty) are derived from the 52 measured features.  
Each calculation were performed within a time-window in which the exercise was performed with a 0.5 seconds overlap between windows. It is not clear whether the calculated values were obtained from unavailble data or if it were obtained from the data within the time-window defined in the dataset by the num_window column. In the former case the values can be recomputed, but in that case an exercise can only be identified if a whole time-window is provided (context). An isolated data point (test case) usually can not give enough information to infer if the exercise is being properly executed (similar to evaluating the performance of an exercise through a snapshot instead of a movie).
We can see that the necessary context is provided withing the testing dataset:
```{r}
setdiff(testingraw$num_window,trainingraw$num_window)
```
All the window numbers from the test set is contained within the training set, which is weird since 
the class can be completely determined by the window_num. The proper exercise would be to give a whole new window to calculate the derived features. Let's assume that we don't know the class of the test dataset, then we will imput the derived feature from the window obtained in the training set into the test set.
From the data we can see that 90% of the window size is about 0.9 seconds:
```{r}
explore=trainingraw
explore$pt=as.double(explore$raw_timestamp_part_1)+explore$raw_timestamp_part_2/1000000
h=hist(unique(explore[,.(pt,window_size=max(pt)-min(pt)),by=num_window])$window_size,breaks = 4,plot=F)
h$xname="Window Size Histogram"
plot(h)
data.table(percentage=h$counts/sum(h$counts)*100, window_size=h$mids)
```
From the literature the optimal window size is about 2.5 seconds. For optimal performance the window should be recalculated.
Lets filter only the raw features from the dataset in order to calculate the derived columns:
```{r,cache=F}
notderived=setdiff(1:160,grep("avg|std|var|kurt|skew|min|max|ampl",incomplete.dt$desc))
source("./functions.R")
pre_training=imput.values(trainingraw)
cnames=colnames(pre_training)
selectedFeatures=select.features(cnames)
#Only 17 features are selected by this function (See Review)
trainings=select(pre_training,one_of(selectedFeatures+"num_window"))
trainings=unique(merge(select(trainingraw,num_window,classe),
                       trainings,all=T,by="num_window"))
```
# Training
A procedure close to the one in this [link](https://www.r-bloggers.com/predictive-modelling-fun-with-the-caret-package/) was used to train the data. The random forest algorithm was use together with the caret package.
First, the num_windows column and rows with incomplete data are removed. Next the data is partitioned (75% training, 25% testing), then a 10 fold cross-validation is performed and the procedure is repeated 5 times, each time a different forest is constructed randomly. The confusion matrix for the training data is plotted at the end.

```{r,cache=T}
set.seed(814)
suppressMessages(library(randomForest))
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(caret))

training=select(trainings,-num_window)
training=training[complete.cases(training)]
ctrl = trainControl(method="repeatedcv", number=10, repeats=5, selectionFunction = "oneSE")
in_train = createDataPartition(training$classe, p=.75, list=FALSE)
trf = train(data=training, classe ~ .,method="rf",
            metric="Kappa", trcontrol=ctrl, subset=in_train)
t.table=trf$finalModel$confusion
pt=plot.confusion.matrix(t.table)
confusion=round(pt[[1]],digits=2)
plot(pt[[2]])
acct=as.data.table(round(trf$results,digits=2) )[1]

```
 The expected accuracy and out-of-sample error is `r acct$Accuracy` +- `r acct$AccuracySD`, and `r acct$Kappa` +- `r acct$KappaSD` for the Kappa estimate.
 
 
# Validation


```{r,cache=T}
#set.seed(1814)
test= as.data.frame(training)[-in_train,]
pred.rf = predict(trf, test, "raw")
c=confusionMatrix(pred.rf, test$classe)
c
acc=as.data.table(t(round(c$overall,digits = 2)))
pt2=plot.confusion.matrix(c$table)
con=round(pt2[[1]],digits=2)
plot(pt2[[2]])
```

We see an optimistic result in the confusion matrix of the training set, as usual, the accuracy lowers as we try to fit our model to unseen data. We can that the Precision for class A drops from `r confusion[1,][[1]]`% to `r con[1,][[1]]`%. The overall accuracy is about `r acc$Accuracy`% (`r acc$AccuracyLower`,`r acc$AccuracyUpper`), 95% CI. Even though there is an imbalance in the data, the reported balanced accuracy of the classes are still high. The accuracy obtained in this report is far lower than the one obtained in the original article (98.2 %), there are several factors that could be responsable for this:

* Unbalanced classed
* Sub-optimal Window sizes (1,0s instead of 2.5s)
* Probably different imputing criteria
* Reduced Sample (19622 samples against 39242)
* Different number of forests (500 forest against 10)

# Test set
In the real dataset you must accquire a lot a data to create a window to which you apply the derived features ( see imput.value function). In this test set each problem provides only a single instance instead a 1,0s window worth of data. In that case, we retrieve the derived features using the num_window as a key. We could even retrieve the original class, since the num_window in the test set is contained in the training set, but we restrict to the derived columns in order to compare how the model would perform with real data. Since we are using data from the training set, we are actually getting a biased result. The alternative would be to model based on the raw features, contrary to the oriented in the original article. The model in that case would underperform.

## Imputing test set with data from the training dataset
```{r ,cache=T}
suppressMessages(library(randomForest))
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(caret))

 testset=unique(merge(select(testingraw,problem_id,num_window),
                      trainings,by="num_window"))
setorder(testset,problem_id)
pred.test.rf = predict(trf, testset, "raw")
result=select(testset,problem_id,classe)
result$predicted.classe=pred.test.rf
result$classe=as.factor(result$classe)
result$isequal=pred.test.rf==result$classe
result
```
That way we got `r sum(result$isequal)` out 20 cases correct, `r sum(result$isequal)/20*100`% accuracy, that is expected since we used data from the training set to imput the derived features of the test set. The validation part is a more reliable result for the accuracy of this model since the random forest is applied to completely unseen data.

# Conclusion
In this report a random forest algorithm was applied to the Human Activity Recognition problem of identifying how well an user performs an exercise. It was possible to construct a procedure similar to the one performed by `r Citet(bib, 1)`. Due to time constraints and incomplete data the accuracy obtained in this report are inferior to the ones in the literature (`r acct$Accuracy`% against 98% accuracy). However, if observed alone,  it was a sucessfull aplication of a machine learning algorithm for classification. It was possible to correctly predict the out of sample error of the accuracy within the estimated confidence interval `r acct$Accuracy` +- `r acct$AccuracySD` E (`r acc$AccuracyLower`,`r acc$AccuracyUpper`). The feature selection was not implemented in this report, instead we used the result presented in the article, a whole new array of results could come from different features, due to time limitations the analisys was restricted to the same set of features to enable a possible comparison. There was insufficient time to perform the  leave-one-subject-out cross-validation.

#Bibliography

```{r, results='asis',echo=FALSE}
PrintBibliography(bib)
```
